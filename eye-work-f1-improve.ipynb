{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1512919,"sourceType":"datasetVersion","datasetId":611716,"isSourceIdPinned":false}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"andrewmvd/ocular-disease-recognition-odir5k\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\npath=\"/kaggle/input/ocular-disease-recognition-odir5k\"\nbase_path = path  # ‚Üê use the same variable, do NOT rewrite manually\n\n# Show folder structure\n# for root, dirs, files in os.walk(base_path):\n#     print(root)\n#     break\n\n# !ls -R \"$base_path\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nbase_path = os.path.join(path, \"ODIR-5K\", \"ODIR-5K\")\n\nlabels_file = os.path.join(base_path, \"data.xlsx\")\ntrain_images_folder = os.path.join(base_path, \"Training Images\")\ntest_images_folder = os.path.join(base_path, \"Testing Images\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_excel(labels_file)\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch # <-- I've added this line to fix the error\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision.transforms import functional as F\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import multilabel_confusion_matrix, f1_score\n\n# # --- Constants ---\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(f\"Using device: {DEVICE}\")\n\n# IMG_SIZE = 224\n# BATCH_SIZE = 32\n# LEARNING_RATE = 1e-4\n# EPOCHS = 15 # Start with 10, increase if needed\n\n# # Disease categories from your 'data.xlsx'\n# CLASSES = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.models as models\nfrom torchvision import transforms\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# --- Constants ---\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\n\nIMG_SIZE = 384  # Increased for better feature extraction\nBATCH_SIZE = 16  # Reduced for larger images and stability\nLEARNING_RATE = 3e-4\nEPOCHS = 25\nNUM_WORKERS = 4\nCLASSES = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\nNUM_CLASSES = len(CLASSES)\n\n# --- Data Preprocessing ---\ndef prepare_dataframe(df, images_folder):\n    \"\"\"Prepare dataframe with proper labels and image paths\"\"\"\n    data = []\n    \n    for idx, row in df.iterrows():\n        # Get both left and right eye images\n        left_img = f\"{row['ID']}_left.jpg\"\n        right_img = f\"{row['ID']}_right.jpg\"\n        \n        left_path = os.path.join(images_folder, left_img)\n        right_path = os.path.join(images_folder, right_img)\n        \n        # Create multi-label vector\n        labels = [\n            int(row['N']), int(row['D']), int(row['G']), int(row['C']),\n            int(row['A']), int(row['H']), int(row['M']), int(row['O'])\n        ]\n        \n        # Add both eyes as separate samples\n        if os.path.exists(left_path):\n            data.append({'image_path': left_path, 'labels': labels})\n        if os.path.exists(right_path):\n            data.append({'image_path': right_path, 'labels': labels})\n    \n    return pd.DataFrame(data)\n\n# --- Custom Dataset ---\nclass OcularDataset(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.df = dataframe.reset_index(drop=True)\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_path = self.df.loc[idx, 'image_path']\n        labels = np.array(self.df.loc[idx, 'labels'], dtype=np.float32)\n        \n        # Read image\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # Apply augmentations\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        \n        return image, torch.tensor(labels, dtype=torch.float32)\n\n# --- Augmentations ---\ndef get_train_transforms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.3),\n        A.Rotate(limit=15, p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n        A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n        A.CLAHE(clip_limit=2.0, p=0.5),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ndef get_val_transforms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\n# --- Model Architecture ---\nclass OcularDiseaseModel(nn.Module):\n    def __init__(self, num_classes=8, pretrained=True):\n        super(OcularDiseaseModel, self).__init__()\n        \n        # Use EfficientNet-B3 for better accuracy\n        self.backbone = models.efficientnet_b3(pretrained=pretrained)\n        \n        # Get number of features\n        num_features = self.backbone.classifier[1].in_features\n        \n        # Replace classifier\n        self.backbone.classifier = nn.Sequential(\n            nn.Dropout(0.3),\n            nn.Linear(num_features, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n    \n    def forward(self, x):\n        return self.backbone(x)\n\n# --- Focal Loss for handling class imbalance ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.25, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def forward(self, inputs, targets):\n        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-bce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n        return focal_loss.mean()\n\n# --- Training Function ---\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    pbar = tqdm(dataloader, desc='Training')\n    for images, labels in pbar:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        # Get predictions\n        preds = torch.sigmoid(outputs) > 0.5\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n        \n        pbar.set_postfix({'loss': loss.item()})\n    \n    epoch_loss = running_loss / len(dataloader)\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    \n    # Calculate metrics\n    f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    return epoch_loss, f1, accuracy\n\n# --- Validation Function ---\ndef validate_epoch(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        pbar = tqdm(dataloader, desc='Validation')\n        for images, labels in pbar:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            \n            # Get predictions\n            preds = torch.sigmoid(outputs) > 0.5\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n            pbar.set_postfix({'loss': loss.item()})\n    \n    epoch_loss = running_loss / len(dataloader)\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    \n    # Calculate metrics\n    f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n    accuracy = accuracy_score(all_labels, all_preds)\n    \n    return epoch_loss, f1, accuracy, all_preds, all_labels\n\n# --- Main Training Pipeline ---\ndef main(base_path):\n    # Load data\n    labels_file = os.path.join(base_path, \"data.xlsx\")\n    train_images_folder = os.path.join(base_path, \"Training Images\")\n    test_images_folder = os.path.join(base_path, \"Testing Images\")\n    \n    print(\"Loading data...\")\n    df = pd.read_excel(labels_file)\n    \n    # Prepare training data\n    train_df = prepare_dataframe(df, train_images_folder)\n    print(f\"Total training samples: {len(train_df)}\")\n    \n    # Split into train and validation\n    train_data, val_data = train_test_split(train_df, test_size=0.15, random_state=42)\n    print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n    \n    # Create datasets\n    train_dataset = OcularDataset(train_data, transform=get_train_transforms())\n    val_dataset = OcularDataset(val_data, transform=get_val_transforms())\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                            num_workers=NUM_WORKERS, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n                          num_workers=NUM_WORKERS, pin_memory=True)\n    \n    # Initialize model\n    print(\"\\nInitializing model...\")\n    model = OcularDiseaseModel(num_classes=NUM_CLASSES, pretrained=True).to(DEVICE)\n    \n    # Loss and optimizer\n    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=3, verbose=True\n    )\n    \n    # Training loop\n    best_f1 = 0.0\n    history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': [], \n               'train_acc': [], 'val_acc': []}\n    \n    print(\"\\nStarting training...\")\n    for epoch in range(EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n        print(\"-\" * 50)\n        \n        # Train\n        train_loss, train_f1, train_acc = train_epoch(\n            model, train_loader, criterion, optimizer, DEVICE\n        )\n        \n        # Validate\n        val_loss, val_f1, val_acc, val_preds, val_labels = validate_epoch(\n            model, val_loader, criterion, DEVICE\n        )\n        \n        # Update scheduler\n        scheduler.step(val_f1)\n        \n        # Save history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_f1'].append(train_f1)\n        history['val_f1'].append(val_f1)\n        history['train_acc'].append(train_acc)\n        history['val_acc'].append(val_acc)\n        \n        print(f\"\\nTrain Loss: {train_loss:.4f} | Train F1: {train_f1:.4f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f} | Val Acc: {val_acc:.4f}\")\n        \n        # Save best model\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'best_f1': best_f1,\n            }, 'best_ocular_model.pth')\n            print(f\"‚úì Best model saved with F1: {best_f1:.4f}\")\n    \n    # Plot training history\n    plot_history(history)\n    \n    # Final evaluation\n    print(\"\\n\" + \"=\"*50)\n    print(\"FINAL RESULTS\")\n    print(\"=\"*50)\n    print(f\"Best Validation F1 Score: {best_f1:.4f}\")\n    print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n    \n    # Per-class metrics\n    print(\"\\nPer-Class Performance:\")\n    print(\"-\" * 50)\n    for i, class_name in enumerate(CLASSES):\n        class_f1 = f1_score(val_labels[:, i], val_preds[:, i], zero_division=0)\n        print(f\"{class_name}: F1 = {class_f1:.4f}\")\n    \n    return model, history\n\ndef plot_history(history):\n    \"\"\"Plot training history\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n    \n    # Loss\n    axes[0].plot(history['train_loss'], label='Train Loss')\n    axes[0].plot(history['val_loss'], label='Val Loss')\n    axes[0].set_title('Loss Over Epochs')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True)\n    \n    # F1 Score\n    axes[1].plot(history['train_f1'], label='Train F1')\n    axes[1].plot(history['val_f1'], label='Val F1')\n    axes[1].set_title('F1 Score Over Epochs')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('F1 Score')\n    axes[1].legend()\n    axes[1].grid(True)\n    \n    # Accuracy\n    axes[2].plot(history['train_acc'], label='Train Acc')\n    axes[2].plot(history['val_acc'], label='Val Acc')\n    axes[2].set_title('Accuracy Over Epochs')\n    axes[2].set_xlabel('Epoch')\n    axes[2].set_ylabel('Accuracy')\n    axes[2].legend()\n    axes[2].grid(True)\n    \n    plt.tight_layout()\n    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# --- Run Training ---\nif __name__ == \"__main__\":\n    # Set your base path\n    base_path = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K\"\n    \n    # Train model\n    model, history = main(base_path)\n    \n    print(\"\\nTraining completed! Best model saved as 'best_ocular_model.pth'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# ODIR-5K Multi-label Eye Disease Classification (F1>0.85 Ready)\n# Author: Atharva M. ‚Äî Optimized Minimal High-Performance Version\n# ================================================================\n\n!pip install --quiet timm albumentations opencv-python-headless torchmetrics openpyxl\n\nimport os, cv2, torch, timm, warnings\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- Constants ----------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\nIMG_SIZE = 384\nBATCH_SIZE = 4\nLR = 3e-5\nEPOCHS = 15\nCLASSES = ['N','D','G','C','A','H','M','O']\nNUM_CLASSES = len(CLASSES)\n\n# ---------------- Dataset Prep ----------------\nBASE_PATH = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K\"\nLABELS_FILE = os.path.join(BASE_PATH, \"data.xlsx\")\nTRAIN_IMG = os.path.join(BASE_PATH, \"Training Images\")\n\ndf = pd.read_excel(LABELS_FILE)\n\ndef prepare_df(df, img_folder):\n    data = []\n    for _, r in df.iterrows():\n        labels = [int(r[c]) for c in CLASSES]\n        for side in [\"left\", \"right\"]:\n            path = os.path.join(img_folder, f\"{r['ID']}_{side}.jpg\")\n            if os.path.exists(path):\n                data.append({\"path\": path, \"labels\": labels})\n    return pd.DataFrame(data)\n\ndf_all = prepare_df(df, TRAIN_IMG)\ntrain_df, val_df = train_test_split(df_all, test_size=0.2, random_state=42, shuffle=True)\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n\n# ---------------- Augmentations ----------------\ndef get_train_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(0.3,0.3,p=0.5),\n        A.HueSaturationValue(10,15,10,p=0.4),\n        A.GaussianBlur(3,p=0.3),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\ndef get_val_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\n\n# ---------------- Dataset ----------------\nclass EyeDataset(Dataset):\n    def __init__(self, df, tfm):\n        self.df, self.tfm = df, tfm\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row[\"path\"]); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.tfm(image=img)[\"image\"]\n        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n        return img, labels\n\ntrain_set = EyeDataset(train_df, get_train_tfms())\nval_set   = EyeDataset(val_df, get_val_tfms())\n\n# Weighted sampler for balancing\nlabel_array = np.stack(train_df[\"labels\"].values)\nsample_weights = 1.0 / (label_array.sum(axis=1) + 1e-3)\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\nval_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# ---------------- Model ----------------\nclass ODIRModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\"convnext_base\", pretrained=True, num_classes=0)\n        n_feats = self.backbone.num_features\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(n_feats, 512),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(512, NUM_CLASSES)\n        )\n    def forward(self,x):\n        f = self.backbone(x)\n        return self.classifier(f)\n\nmodel = ODIRModel().to(DEVICE)\n\n# ---------------- Loss & Optimizer ----------------\nclass AsymmetricLoss(torch.nn.Module):\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05):\n        super().__init__()\n        self.gn, self.gp, self.clip = gamma_neg, gamma_pos, clip\n    def forward(self, x, y):\n        xs = torch.sigmoid(x)\n        xs_pos, xs_neg = xs, 1-xs\n        if self.clip: xs_neg = torch.clamp(xs_neg+self.clip, max=1)\n        loss_pos = y*torch.log(xs_pos+1e-8)\n        loss_neg = (1-y)*torch.log(xs_neg+1e-8)\n        loss = loss_pos + loss_neg\n        pt = xs_pos*y + xs_neg*(1-y)\n        one_side = self.gp*y + self.gn*(1-y)\n        loss *= (1-pt)**one_side\n        return -loss.mean()\n\ncriterion = AsymmetricLoss()\noptimizer = torch.optim.AdamW([\n    {\"params\": model.backbone.parameters(), \"lr\": LR*0.3},\n    {\"params\": model.classifier.parameters(), \"lr\": LR}\n], weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=[LR*0.3, LR], steps_per_epoch=len(train_loader), epochs=EPOCHS\n)\nscaler = torch.cuda.amp.GradScaler()\n\n# ---------------- Training ----------------\ndef epoch_loop(loader, train=True):\n    model.train() if train else model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0\n    pbar = tqdm(loader, desc=\"Train\" if train else \"Val\", leave=False)\n    for imgs, labels in pbar:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=True):\n            out = model(imgs)\n            loss = criterion(out, labels)\n        if train:\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer); scaler.update(); scheduler.step()\n        total_loss += loss.item()\n        preds = (torch.sigmoid(out) > 0.5).float().cpu().numpy()\n        all_preds.extend(preds); all_labels.extend(labels.cpu().numpy())\n        pbar.set_postfix(loss=loss.item())\n    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    acc = accuracy_score(np.all(np.array(all_labels)==np.array(all_preds), axis=1),\n                         np.ones(len(all_labels)))\n    return total_loss/len(loader), f1_micro, f1_macro, acc\n\nbest_f1 = 0\nfor epoch in range(1, EPOCHS+1):\n    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n    tr_loss,tr_f1,tr_f1m,tr_acc = epoch_loop(train_loader, True)\n    vl_loss,vl_f1,vl_f1m,vl_acc = epoch_loop(val_loader, False)\n    print(f\"Train: Loss {tr_loss:.4f} | F1¬µ {tr_f1:.3f} | F1M {tr_f1m:.3f}\")\n    print(f\"Val  : Loss {vl_loss:.4f} | F1¬µ {vl_f1:.3f} | F1M {vl_f1m:.3f}\")\n    if vl_f1 > best_f1:\n        best_f1 = vl_f1\n        torch.save(model.state_dict(), \"best_ODIR_model.pth\")\n        print(f\"‚úÖ Saved new best model (F1¬µ={best_f1:.3f})\")\n\nprint(\"\\nüéØ Training complete! Best F1-Micro:\", round(best_f1,4))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"best_ODIR_model.pth\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- FIX: ensure Albumentations and SciPy are compatible ---\n!pip uninstall -y scipy\n!pip install --no-cache-dir \"scipy==1.10.1\" albumentations==1.4.3\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install --upgrade --force-reinstall numpy scipy scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# ODIR-5K Dual CNN (F1 ‚â• 0.85) - Robust Version with Native PyTorch\n# ================================================================\n!pip install --quiet timm openpyxl pillow scikit-learn\n\nimport os, warnings, torch, timm\nimport numpy as np, pandas as pd\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\nimport torchvision.transforms as T\nwarnings.filterwarnings(\"ignore\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMG_SIZE, BATCH_SIZE, EPOCHS, LR = 384, 6, 25, 5e-5\nCLASSES = ['N','D','G','C','A','H','M','O']\nNUM_CLASSES = len(CLASSES)\nWARMUP_EPOCHS = 3\nprint(\"Device:\", DEVICE)\n\n# ----------------- Load Data for DUAL EYES -----------------\nBASE = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K\"\ndf = pd.read_excel(os.path.join(BASE,\"data.xlsx\"))\n\ndef build_dual_df(df):\n    rows=[]\n    for _,r in df.iterrows():\n        labels=[int(r[c]) for c in CLASSES]\n        left_p=os.path.join(BASE,\"Training Images\",f\"{r['ID']}_left.jpg\")\n        right_p=os.path.join(BASE,\"Training Images\",f\"{r['ID']}_right.jpg\")\n        if os.path.exists(left_p) and os.path.exists(right_p):\n            rows.append({\"left_path\":left_p,\"right_path\":right_p,\"labels\":labels})\n    return pd.DataFrame(rows)\n\ndf_all=build_dual_df(df)\ntrain_df,val_df=train_test_split(df_all,test_size=0.15,random_state=42,shuffle=True)\nprint(len(train_df),\"train pairs,\",len(val_df),\"val pairs\")\n\n# ----------------- Native PyTorch Augmentations -----------------\nclass TrainTransform:\n    def __init__(self, size=384):\n        self.size = size\n        self.normalize = T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n        \n    def __call__(self, img):\n        # Random horizontal flip\n        if torch.rand(1) < 0.5:\n            img = T.functional.hflip(img)\n        \n        # Random vertical flip\n        if torch.rand(1) < 0.2:\n            img = T.functional.vflip(img)\n        \n        # Color jitter\n        if torch.rand(1) < 0.6:\n            brightness = 1 + (torch.rand(1) - 0.5) * 0.6\n            contrast = 1 + (torch.rand(1) - 0.5) * 0.6\n            img = T.functional.adjust_brightness(img, brightness.item())\n            img = T.functional.adjust_contrast(img, contrast.item())\n        \n        # Random rotation\n        if torch.rand(1) < 0.6:\n            angle = (torch.rand(1) - 0.5) * 24\n            img = T.functional.rotate(img, angle.item())\n        \n        # Convert to tensor and normalize\n        img = T.functional.to_tensor(img)\n        img = self.normalize(img)\n        \n        return img\n\nclass ValTransform:\n    def __init__(self, size=384):\n        self.size = size\n        self.normalize = T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n        \n    def __call__(self, img):\n        img = T.functional.to_tensor(img)\n        img = self.normalize(img)\n        return img\n\n# ----------------- Dual Eye Dataset -----------------\nclass DualEyeDataset(Dataset):\n    def __init__(self,df,transform,size=384):\n        self.df=df.reset_index(drop=True)\n        self.transform=transform\n        self.size=size\n    \n    def __len__(self): \n        return len(self.df)\n    \n    def __getitem__(self,i):\n        r=self.df.iloc[i]\n        \n        # Load and resize left eye\n        left_img=Image.open(r[\"left_path\"]).convert('RGB')\n        left_img=left_img.resize((self.size,self.size),Image.BILINEAR)\n        left_img=self.transform(left_img)\n        \n        # Load and resize right eye\n        right_img=Image.open(r[\"right_path\"]).convert('RGB')\n        right_img=right_img.resize((self.size,self.size),Image.BILINEAR)\n        right_img=self.transform(right_img)\n        \n        y=torch.tensor(r[\"labels\"],dtype=torch.float32)\n        return left_img,right_img,y\n\ntrain_ds=DualEyeDataset(train_df,TrainTransform(IMG_SIZE),IMG_SIZE)\nval_ds=DualEyeDataset(val_df,ValTransform(IMG_SIZE),IMG_SIZE)\n\nlabel_arr=np.stack(train_df[\"labels\"].values)\npos_counts=label_arr.sum(axis=0)\nneg_counts=len(label_arr)-pos_counts\npos_weights=neg_counts/np.maximum(pos_counts,1)\nweights=1.0/(label_arr.sum(axis=1)+1e-3)\nsampler=WeightedRandomSampler(weights,len(weights),replacement=True)\n\ntrain_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,sampler=sampler,num_workers=2,pin_memory=True)\nval_dl=DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=2,pin_memory=True)\n\n# ----------------- DUAL CNN MODEL -----------------\nclass DualCNNModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Shared backbone for both eyes\n        self.backbone=timm.create_model(\"convnext_small\",pretrained=True,num_classes=0)\n        n=self.backbone.num_features\n        \n        # Individual eye processing branches\n        self.left_branch=torch.nn.Sequential(\n            torch.nn.Linear(n,384),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3)\n        )\n        self.right_branch=torch.nn.Sequential(\n            torch.nn.Linear(n,384),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3)\n        )\n        \n        # Cross-attention between eyes\n        self.cross_attn=torch.nn.MultiheadAttention(384,num_heads=8,dropout=0.2,batch_first=True)\n        \n        # Fusion head\n        self.fusion_head=torch.nn.Sequential(\n            torch.nn.Linear(384*3,512),  # left + right + fused\n            torch.nn.GELU(),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.Dropout(0.4),\n            torch.nn.Linear(512,256),\n            torch.nn.GELU(),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(256,NUM_CLASSES)\n        )\n    \n    def forward(self,left,right):\n        # Extract features from both eyes using shared backbone\n        left_feat=self.backbone(left)\n        right_feat=self.backbone(right)\n        \n        # Process through individual branches\n        left_proc=self.left_branch(left_feat)\n        right_proc=self.right_branch(right_feat)\n        \n        # Cross-attention: let each eye attend to the other\n        left_attn=left_proc.unsqueeze(1)\n        right_attn=right_proc.unsqueeze(1)\n        \n        # Attend left to right\n        left_attended,_=self.cross_attn(left_attn,right_attn,right_attn)\n        left_attended=left_attended.squeeze(1)\n        \n        # Attend right to left\n        right_attended,_=self.cross_attn(right_attn,left_attn,left_attn)\n        right_attended=right_attended.squeeze(1)\n        \n        # Fuse information\n        fused=(left_attended+right_attended)/2\n        \n        # Concatenate all representations\n        combined=torch.cat([left_proc,right_proc,fused],dim=1)\n        \n        return self.fusion_head(combined)\n\nmodel=DualCNNModel().to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n\n# ----------------- Loss Functions -----------------\nclass FocalLoss(torch.nn.Module):\n    def __init__(self,alpha=0.25,gamma=2):\n        super().__init__();self.alpha=alpha;self.gamma=gamma\n    def forward(self,x,y):\n        bce=F.binary_cross_entropy_with_logits(x,y,reduction='none')\n        pt=torch.exp(-bce)\n        focal=(1-pt)**self.gamma*bce\n        return (self.alpha*y*focal+(1-self.alpha)*(1-y)*focal).mean()\n\nfocal=FocalLoss(alpha=0.3,gamma=2.5)\nbce_weighted=torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights,device=DEVICE).float())\n\ndef hybrid_loss(x,y): \n    return 0.5*bce_weighted(x,y)+0.5*focal(x,y)\n\n# ----------------- Optimizer -----------------\nopt=torch.optim.AdamW([\n    {\"params\":model.backbone.parameters(),\"lr\":LR*0.1},\n    {\"params\":model.left_branch.parameters(),\"lr\":LR*0.5},\n    {\"params\":model.right_branch.parameters(),\"lr\":LR*0.5},\n    {\"params\":model.cross_attn.parameters(),\"lr\":LR*0.7},\n    {\"params\":model.fusion_head.parameters(),\"lr\":LR}\n],weight_decay=2e-5)\n\ndef get_lr(epoch):\n    if epoch<WARMUP_EPOCHS: return (epoch+1)/WARMUP_EPOCHS\n    return 0.5*(1+np.cos(np.pi*(epoch-WARMUP_EPOCHS)/(EPOCHS-WARMUP_EPOCHS)))\n\nsched=torch.optim.lr_scheduler.LambdaLR(opt,lr_lambda=get_lr)\nscaler=torch.cuda.amp.GradScaler()\n\n# ----------------- Training -----------------\nbest_f1=0\naccum_steps=2\n\nfor epoch in range(1,EPOCHS+1):\n    model.train();tloss,allp,alll=0,[],[]\n    pbar=tqdm(train_dl,desc=f\"Epoch {epoch}/{EPOCHS}\")\n    opt.zero_grad()\n    \n    for step,(left,right,y) in enumerate(pbar):\n        left,right,y=left.to(DEVICE),right.to(DEVICE),y.to(DEVICE)\n        \n        with torch.cuda.amp.autocast():\n            out=model(left,right)\n            loss=hybrid_loss(out,y)/accum_steps\n        \n        scaler.scale(loss).backward()\n        \n        if (step+1)%accum_steps==0 or step==len(train_dl)-1:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n            scaler.step(opt);scaler.update();opt.zero_grad()\n        \n        tloss+=loss.item()*left.size(0)*accum_steps\n        allp.append(torch.sigmoid(out).detach().cpu().numpy())\n        alll.append(y.cpu().numpy())\n        pbar.set_postfix({\"loss\":f\"{loss.item()*accum_steps:.4f}\"})\n    \n    sched.step()\n    allp,alll=np.vstack(allp),np.vstack(alll)\n    preds=(allp>0.5).astype(int)\n    trf1=f1_score(alll,preds,average='micro')\n    \n    # Validation\n    model.eval();vloss,vlp,vll=0,[],[]\n    with torch.no_grad():\n        for left,right,y in val_dl:\n            left,right,y=left.to(DEVICE),right.to(DEVICE),y.to(DEVICE)\n            with torch.cuda.amp.autocast():\n                o=model(left,right)\n            vloss+=hybrid_loss(o,y).item()*left.size(0)\n            vlp.append(torch.sigmoid(o).cpu().numpy())\n            vll.append(y.cpu().numpy())\n    \n    vlp,vll=np.vstack(vlp),np.vstack(vll)\n    val_preds=(vlp>0.5).astype(int)\n    vf1=f1_score(vll,val_preds,average='micro')\n    vf1m=f1_score(vll,val_preds,average='macro')\n    print(f\"Train F1¬µ {trf1:.3f} | Val F1¬µ {vf1:.3f} | Val F1M {vf1m:.3f}\")\n    \n    # Per-class F1\n    f1s=[f1_score(vll[:,i],val_preds[:,i],zero_division=0) for i in range(NUM_CLASSES)]\n    print(\"Per-class F1:\",{CLASSES[i]:round(f1s[i],3) for i in range(NUM_CLASSES)})\n    \n    if vf1>best_f1:\n        best_f1=vf1\n        torch.save(model.state_dict(),\"best_dual_f1_model.pth\")\n        print(f\"‚úÖ Saved new best (F1¬µ={best_f1:.3f})\")\n\nprint(\"\\nüéØ Best Validation F1-micro:\",best_f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# ODIR-5K Multi-label Eye Disease Classification (F1>0.75+)\n# UPGRADED VERSION: Swin Transformer + CLAHE + ASL\n# ===============================================================\n\n# !pip install --quiet timm albumentations opencv-python-headless torchmetrics openpyxl\n\nimport os, cv2, torch, timm, warnings\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- Constants ----------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\nIMG_SIZE = 384\nBATCH_SIZE = 4   # Keep batch size small for Swin-Base @ 384px\nLR = 3e-5        # Good starting LR for fine-tuning transformers\nEPOCHS = 20      # Increased epochs for larger model\nCLASSES = ['N','D','G','C','A','H','M','O']\nNUM_CLASSES = len(CLASSES)\n\n# ---------------- Dataset Prep ----------------\nBASE_PATH = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K\"\nLABELS_FILE = os.path.join(BASE_PATH, \"data.xlsx\")\nTRAIN_IMG = os.path.join(BASE_PATH, \"Training Images\")\n\ndf = pd.read_excel(LABELS_FILE)\n\ndef prepare_df(df, img_folder):\n    data = []\n    for _, r in df.iterrows():\n        labels = [int(r[c]) for c in CLASSES]\n        for side in [\"left\", \"right\"]:\n            path = os.path.join(img_folder, f\"{r['ID']}_{side}.jpg\")\n            if os.path.exists(path):\n                data.append({\"path\": path, \"labels\": labels})\n    return pd.DataFrame(data)\n\ndf_all = prepare_df(df, TRAIN_IMG)\ntrain_df, val_df = train_test_split(df_all, test_size=0.2, random_state=42, shuffle=True)\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n\n# ---------------- Augmentations (with CLAHE) ----------------\ndef get_train_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        # This is the key addition for this dataset\n        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.8),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(0.3,0.3,p=0.5),\n        A.HueSaturationValue(10,15,10,p=0.4),\n        A.GaussianBlur(3,p=0.3),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\ndef get_val_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        # Apply CLAHE to validation too\n        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\n\n# ---------------- Dataset ----------------\nclass EyeDataset(Dataset):\n    def __init__(self, df, tfm):\n        self.df, self.tfm = df, tfm\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = cv2.imread(row[\"path\"]); img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.tfm(image=img)[\"image\"]\n        labels = torch.tensor(row[\"labels\"], dtype=torch.float32)\n        return img, labels\n\ntrain_set = EyeDataset(train_df, get_train_tfms())\nval_set   = EyeDataset(val_df, get_val_tfms())\n\n# Weighted sampler for balancing\nlabel_array = np.stack(train_df[\"labels\"].values)\nsample_weights = 1.0 / (label_array.sum(axis=1) + 1e-3)\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\ntrain_loader = DataLoader(train_set, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2)\nval_loader   = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# ---------------- Model (Swin Transformer) ----------------\nclass ODIRModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Using Swin-Base pre-trained at 384px\n        self.backbone = timm.create_model(\n            \"swin_base_patch4_window12_384\", \n            pretrained=True, \n            num_classes=0, # Remove head\n            in_chans=3\n        )\n        n_feats = self.backbone.num_features # This is 1024 for Swin-B\n        \n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(n_feats, 512),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(512, NUM_CLASSES)\n        )\n    def forward(self,x):\n        f = self.backbone(x)\n        return self.classifier(f)\n\nmodel = ODIRModel().to(DEVICE)\n\n# ---------------- Loss & Optimizer ----------------\nclass AsymmetricLoss(torch.nn.Module):\n    # This is the ASL loss from your Cell 8, it's excellent\n    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05):\n        super().__init__()\n        self.gn, self.gp, self.clip = gamma_neg, gamma_pos, clip\n    def forward(self, x, y):\n        xs = torch.sigmoid(x)\n        xs_pos, xs_neg = xs, 1-xs\n        if self.clip: xs_neg = torch.clamp(xs_neg+self.clip, max=1)\n        loss_pos = y*torch.log(xs_pos+1e-8)\n        loss_neg = (1-y)*torch.log(xs_neg+1e-8)\n        loss = loss_pos + loss_neg\n        pt = xs_pos*y + xs_neg*(1-y)\n        one_side = self.gp*y + self.gn*(1-y)\n        loss *= (1-pt)**one_side\n        return -loss.mean()\n\ncriterion = AsymmetricLoss()\noptimizer = torch.optim.AdamW([\n    # Lower LR for the pre-trained backbone\n    {\"params\": model.backbone.parameters(), \"lr\": LR*0.3},\n    # Higher LR for the new classifier head\n    {\"params\": model.classifier.parameters(), \"lr\": LR}\n], weight_decay=1e-5)\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, max_lr=[LR*0.3, LR], steps_per_epoch=len(train_loader), epochs=EPOCHS\n)\nscaler = torch.cuda.amp.GradScaler()\n\n# ---------------- Training ----------------\ndef epoch_loop(loader, train=True):\n    model.train() if train else model.eval()\n    all_preds, all_labels = [], []\n    total_loss = 0\n    pbar = tqdm(loader, desc=\"Train\" if train else \"Val\", leave=False)\n    \n    for imgs, labels in pbar:\n        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n        optimizer.zero_grad()\n        \n        with torch.cuda.amp.autocast(enabled=True):\n            out = model(imgs)\n            loss = criterion(out, labels)\n            \n        if train:\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer); scaler.update(); scheduler.step()\n            \n        total_loss += loss.item()\n        preds = (torch.sigmoid(out) > 0.5).float().cpu().numpy()\n        all_preds.extend(preds); all_labels.extend(labels.cpu().numpy())\n        pbar.set_postfix(loss=loss.item())\n        \n    all_labels = np.array(all_labels)\n    all_preds = np.array(all_preds)\n    \n    f1_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n    f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    \n    return total_loss/len(loader), f1_micro, f1_macro\n\n# --- Training Loop ---\nbest_f1 = 0\nfor epoch in range(1, EPOCHS+1):\n    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n    tr_loss,tr_f1,tr_f1m = epoch_loop(train_loader, True)\n    vl_loss,vl_f1,vl_f1m = epoch_loop(val_loader, False)\n    \n    print(f\"Train: Loss {tr_loss:.4f} | F1¬µ {tr_f1:.3f} | F1M {tr_f1m:.3f}\")\n    print(f\"Val  : Loss {vl_loss:.4f} | F1¬µ {vl_f1:.3f} | F1M {vl_f1m:.3f}\")\n    \n    if vl_f1 > best_f1:\n        best_f1 = vl_f1\n        torch.save(model.state_dict(), \"best_Swin_model.pth\")\n        print(f\"‚úÖ Saved new best model (F1¬µ={best_f1:.3f})\")\n\nprint(f\"\\nüéØ Training complete! Best F1-Micro: {best_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:03:37.481414Z","iopub.execute_input":"2025-11-13T10:03:37.481727Z","iopub.status.idle":"2025-11-13T10:04:11.736922Z","shell.execute_reply.started":"2025-11-13T10:03:37.481698Z","shell.execute_reply":"2025-11-13T10:04:11.734978Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTrain: 5600, Val: 1400\n\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"                                                                     \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3371736292.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch}/{EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_f1m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mvl_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvl_f1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvl_f1m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3371736292.py\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(loader, train)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ===============================================================\n# ODIR-5K Dual Swin Transformer (F1>0.75+)\n# Architecture: Dual-Eye Cross-Attention (from your Cell 12)\n# Backbone: Swin Transformer (swin_base_patch4_window12_384)\n# Augs: CLAHE + ASL (from my suggestion)\n# ===============================================================\n\n!pip install --quiet timm albumentations opencv-python-headless torchmetrics openpyxl\n\nimport os, cv2, torch, timm, warnings\nimport pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nwarnings.filterwarnings(\"ignore\")\n\n# ---------------- Constants ----------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", DEVICE)\nIMG_SIZE = 384\nBATCH_SIZE = 4   # Smaller batch for large Swin-Base model\nACCUM_STEPS = 4  # Increase accumulation for effective batch size of 16\nLR = 5e-5        # Base LR for the heads (from your Cell 12)\nEPOCHS = 20      # Give it 20 epochs\nCLASSES = ['N','D','G','C','A','H','M','O']\nNUM_CLASSES = len(CLASSES)\nWARMUP_EPOCHS = 3 # From your Cell 12\n\n# ---------------- Dataset Prep ----------------\nBASE = \"/kaggle/input/ocular-disease-recognition-odir5k/ODIR-5K/ODIR-5K\"\ndf = pd.read_excel(os.path.join(BASE,\"data.xlsx\"))\n\ndef build_dual_df(df):\n    rows=[]\n    for _,r in df.iterrows():\n        labels=[int(r[c]) for c in CLASSES]\n        left_p=os.path.join(BASE,\"Training Images\",f\"{r['ID']}_left.jpg\")\n        right_p=os.path.join(BASE,\"Training Images\",f\"{r['ID']}_right.jpg\")\n        if os.path.exists(left_p) and os.path.exists(right_p):\n            rows.append({\"left_path\":left_p,\"right_path\":right_p,\"labels\":labels})\n    return pd.DataFrame(rows)\n\ndf_all=build_dual_df(df)\ntrain_df,val_df=train_test_split(df_all,test_size=0.15,random_state=42,shuffle=True)\nprint(len(train_df),\"train pairs,\",len(val_df),\"val pairs\")\n\n# ---------------- Albumentations (with CLAHE) ----------------\ndef get_train_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        # Key addition for this dataset to handle lighting\n        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.8), \n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(0.3,0.3,p=0.5),\n        A.HueSaturationValue(10,15,10,p=0.4),\n        A.GaussianBlur(3,p=0.3),\n        A.Rotate(limit=15, p=0.5),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\n\ndef get_val_tfms():\n    return A.Compose([\n        A.Resize(IMG_SIZE, IMG_SIZE),\n        # Also apply CLAHE to validation\n        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n        A.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n        ToTensorV2()\n    ])\n\n# ---------------- Dual Eye Dataset (using Albumentations) ----------------\nclass DualEyeDataset(Dataset):\n    def __init__(self,df,transform):\n        self.df=df.reset_index(drop=True)\n        self.transform=transform\n    \n    def __len__(self): \n        return len(self.df)\n    \n    def __getitem__(self,i):\n        r=self.df.iloc[i]\n        \n        # Load with CV2 for Albumentations\n        left_img=cv2.imread(r[\"left_path\"])\n        left_img=cv2.cvtColor(left_img, cv2.COLOR_BGR2RGB)\n        \n        right_img=cv2.imread(r[\"right_path\"])\n        right_img=cv2.cvtColor(right_img, cv2.COLOR_BGR2RGB)\n        \n        # Apply transforms\n        left_img = self.transform(image=left_img)[\"image\"]\n        right_img = self.transform(image=right_img)[\"image\"]\n        \n        y=torch.tensor(r[\"labels\"],dtype=torch.float32)\n        return left_img, right_img, y\n\ntrain_ds=DualEyeDataset(train_df, get_train_tfms())\nval_ds=DualEyeDataset(val_df, get_val_tfms())\n\n# --- Sampler (from your Cell 12) ---\nlabel_arr=np.stack(train_df[\"labels\"].values)\npos_counts=label_arr.sum(axis=0)\nneg_counts=len(label_arr)-pos_counts\npos_weights=neg_counts/np.maximum(pos_counts,1)\nweights=1.0/(label_arr.sum(axis=1)+1e-3) # Per-sample weighting\nsampler=WeightedRandomSampler(weights,len(weights),replacement=True)\n\ntrain_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,sampler=sampler,num_workers=2,pin_memory=True)\nval_dl=DataLoader(val_ds,batch_size=BATCH_SIZE,shuffle=False,num_workers=2,pin_memory=True)\n\n# ----------------- DUAL SWIN TRANSFORMER MODEL -----------------\nclass DualSwinModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Shared Swin-Base backbone\n        self.backbone=timm.create_model(\n            \"swin_base_patch4_window12_384\",\n            pretrained=True,\n            num_classes=0,\n            in_chans=3\n        )\n        # n_feats for Swin-Base is 1024\n        n=self.backbone.num_features \n        \n        # Individual eye processing branches (from your Cell 12)\n        self.left_branch=torch.nn.Sequential(\n            torch.nn.Linear(n,384),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3)\n        )\n        self.right_branch=torch.nn.Sequential(\n            torch.nn.Linear(n,384),\n            torch.nn.GELU(),\n            torch.nn.Dropout(0.3)\n        )\n        \n        # Cross-attention (from your Cell 12)\n        self.cross_attn=torch.nn.MultiheadAttention(384,num_heads=8,dropout=0.2,batch_first=True)\n        \n        # Fusion head (from your Cell 12)\n        self.fusion_head=torch.nn.Sequential(\n            torch.nn.Linear(384*3,512),  # left + right + fused\n            torch.nn.GELU(),\n            torch.nn.BatchNorm1d(512),\n            torch.nn.Dropout(0.4),\n            torch.nn.Linear(512,256),\n            torch.nn.GELU(),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.Dropout(0.3),\n            torch.nn.Linear(256,NUM_CLASSES)\n        )\n    \n    def forward(self,left,right):\n        # Extract features (B, C, H, W) -> (B, N)\n        left_feat=self.backbone(left)\n        right_feat=self.backbone(right)\n        \n        # Process through individual branches\n        left_proc=self.left_branch(left_feat)\n        right_proc=self.right_branch(right_feat)\n        \n        # Cross-attention\n        left_attn=left_proc.unsqueeze(1)\n        right_attn=right_proc.unsqueeze(1)\n        \n        left_attended,_=self.cross_attn(left_attn,right_attn,right_attn)\n        left_attended=left_attended.squeeze(1)\n        \n        right_attended,_=self.cross_attn(right_attn,left_attn,left_attn)\n        right_attended=right_attended.squeeze(1)\n        \n        fused=(left_attended+right_attended)/2\n        \n        combined=torch.cat([left_proc,right_proc,fused],dim=1)\n        \n        return self.fusion_head(combined)\n\nmodel=DualSwinModel().to(DEVICE)\nprint(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n# Note: This is a ~178M param model (Swin-B is ~88M x 2 backbones), \n# so BATCH_SIZE=4 and ACCUM_STEPS=4 is a good idea.\n\n# ----------------- Loss Functions (from your Cell 12) -----------------\nclass FocalLoss(torch.nn.Module):\n    def __init__(self,alpha=0.25,gamma=2):\n        super().__init__();self.alpha=alpha;self.gamma=gamma\n    def forward(self,x,y):\n        bce=F.binary_cross_entropy_with_logits(x,y,reduction='none')\n        pt=torch.exp(-bce)\n        focal=(1-pt)**self.gamma*bce\n        return (self.alpha*y*focal+(1-self.alpha)*(1-y)*focal).mean()\n\nfocal=FocalLoss(alpha=0.3,gamma=2.5) # Your params\nbce_weighted=torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weights,device=DEVICE).float())\n\ndef hybrid_loss(x,y): \n    return 0.5*bce_weighted(x,y)+0.5*focal(x,y)\n\n# ----------------- Optimizer & Scheduler (from your Cell 12) -----------------\nopt=torch.optim.AdamW([\n    {\"params\":model.backbone.parameters(),\"lr\":LR*0.1},\n    {\"params\":model.left_branch.parameters(),\"lr\":LR*0.5},\n    {\"params\":model.right_branch.parameters(),\"lr\":LR*0.5},\n    {\"params\":model.cross_attn.parameters(),\"lr\":LR*0.7},\n    {\"params\":model.fusion_head.parameters(),\"lr\":LR}\n],weight_decay=2e-5)\n\ndef get_lr(epoch):\n    if epoch<WARMUP_EPOCHS: return (epoch+1)/WARMUP_EPOCHS\n    return 0.5*(1+np.cos(np.pi*(epoch-WARMUP_EPOCHS)/(EPOCHS-WARMUP_EPOCHS)))\n\nsched=torch.optim.lr_scheduler.LambdaLR(opt,lr_lambda=get_lr)\nscaler=torch.cuda.amp.GradScaler()\n\n# ----------------- Training Loop (from your Cell 12) -----------------\nbest_f1=0\n\nfor epoch in range(1,EPOCHS+1):\n    model.train();tloss,allp,alll=0,[],[]\n    pbar=tqdm(train_dl,desc=f\"Epoch {epoch}/{EPOCHS}\")\n    opt.zero_grad()\n    \n    for step,(left,right,y) in enumerate(pbar):\n        left,right,y=left.to(DEVICE),right.to(DEVICE),y.to(DEVICE)\n        \n        with torch.cuda.amp.autocast():\n            out=model(left,right)\n            loss=hybrid_loss(out,y)/ACCUM_STEPS\n        \n        scaler.scale(loss).backward()\n        \n        if (step+1)%ACCUM_STEPS==0 or step==len(train_dl)-1:\n            scaler.unscale_(opt)\n            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n            scaler.step(opt);scaler.update();opt.zero_grad()\n        \n        tloss+=loss.item()*left.size(0)*ACCUM_STEPS\n        allp.append(torch.sigmoid(out).detach().cpu().numpy())\n        alll.append(y.cpu().numpy())\n        pbar.set_postfix({\"loss\":f\"{loss.item()*ACCUM_STEPS:.4f}\"})\n    \n    sched.step() # Step the scheduler once per epoch\n    allp,alll=np.vstack(allp),np.vstack(alll)\n    preds=(allp>0.5).astype(int)\n    trf1=f1_score(alll,preds,average='micro')\n    \n    # Validation\n    model.eval();vloss,vlp,vll=0,[],[]\n    with torch.no_grad():\n        for left,right,y in val_dl:\n            left,right,y=left.to(DEVICE),right.to(DEVICE),y.to(DEVICE)\n            with torch.cuda.amp.autocast():\n                o=model(left,right)\n            vloss+=hybrid_loss(o,y).item()*left.size(0)\n            vlp.append(torch.sigmoid(o).cpu().numpy())\n            vll.append(y.cpu().numpy())\n    \n    vlp,vll=np.vstack(vlp),np.vstack(vll)\n    val_preds=(vlp>0.5).astype(int)\n    vf1=f1_score(vll,val_preds,average='micro')\n    vf1m=f1_score(vll,val_preds,average='macro')\n    print(f\"Train F1¬µ {trf1:.3f} | Val F1¬µ {vf1:.3f} | Val F1M {vf1m:.3f}\")\n    \n    # Per-class F1\n    f1s=[f1_score(vll[:,i],val_preds[:,i],zero_division=0) for i in range(NUM_CLASSES)]\n    print(\"Per-class F1:\",{CLASSES[i]:round(f1s[i],3) for i in range(NUM_CLASSES)})\n    \n    if vf1>best_f1:\n        best_f1=vf1\n        torch.save(model.state_dict(),\"best_dual_swin_f1_model.pth\")\n        print(f\"‚úÖ Saved new best (F1¬µ={best_f1:.3f})\")\n\nprint(f\"\\nüéØ Best Validation F1-micro: {best_f1:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T10:04:15.833366Z","iopub.execute_input":"2025-11-13T10:04:15.833708Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.2.6 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.2.6 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mUsing device: cuda\n2975 train pairs, 525 val pairs\nModel parameters: 88.98M\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20:  23%|‚ñà‚ñà‚ñé       | 169/744 [01:13<04:02,  2.37it/s, loss=0.8708]","output_type":"stream"}],"execution_count":null}]}